{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Necessary Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data from CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS1 = ['101', '106', '108', '109', '112', '114', '115', '116', '118', '119', '122', '124', \n",
    "       '201', '203', '205', '207', '208', '209', '215', '220', '223', '230']\n",
    "\n",
    "DS2 = ['100', '103', '105', '111', '113', '117', '121', '123', '200', '202', '210', '212', \n",
    "       '213', '214', '219', '221', '222', '228', '231', '232', '233', '234']\n",
    "\n",
    "SUP1 = ['800', '801', '802', '803', '804', '805', '806', '807', '808', '809', '810', '811', '812', '820', '821', \n",
    "       '822', '823', '824', '825', '826', '827', '828', '829', '840', '841', '842', '843', '844', '845', '846', \n",
    "       '847', '848', '849', '850']\n",
    "\n",
    "SUP2 = ['851','852','853','854','855','856','857','858','859','860','861','862','863','864','865','866','867',\n",
    "       '868','869','870','871','872','873','874','875','876','877','878','879','880','881','882','883','884',\n",
    "       '885','886','887','888','889','890','891','892','893','894']\n",
    "\n",
    "INCART = []\n",
    "for i in range(1, 76):    #for the whole INCART database, the number is ranging from 1 to 76\n",
    "    if i < 10:\n",
    "        INCART.append('I0' + str(i))\n",
    "    else:\n",
    "        INCART.append('I' + str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = DS1  #DS1, DS2, SUP1, SUP2, INCART\n",
    "DSName = 'DS1'  #DS1, DS2, SUP, INCART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSet_leads = {}\n",
    "trainingSet_anns = {}\n",
    "for ds in DS:\n",
    "    trainingSet_leads[ds] = pd.read_csv('~/Data/Cleaned Data/' + DSName + '/' + ds + '_lead.csv')\n",
    "    trainingSet_anns[ds] = pd.read_csv('~/Data/Cleaned Data/' + DSName + '/' + ds + '_ann.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heartbeat Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = ['N', 'L', 'R', 'e', 'j']\n",
    "SVEB = ['A', 'a', 'J', 'S']\n",
    "VEB = ['V', 'E']\n",
    "F = ['F']\n",
    "Q = ['l', 'f', 'Q']\n",
    "Non_beat_anns = ['[', ']', '!', 'x', '(', ')', 'p', 't', 'u', '`', '~', '^', '|', '+', 's', 'T', '*', 'D', '=', '\"', '@']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heatbeat Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each heartbeat knowing the R location, we take samples from the interval of 250 ms before R peak and 400 ms after R peak, i.e. total 0.65 s of each heartbeat. That is, 90 samples before R peak and 144 samples after R peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hbs = {}\n",
    "for ds in DS:\n",
    "    lead0 = trainingSet_leads[ds]['lead0']\n",
    "    lead1 = trainingSet_leads[ds]['lead1']\n",
    "    hbs0 = []\n",
    "    hbs1 = []\n",
    "    anns = []\n",
    "    annIdxs = []\n",
    "    for row in trainingSet_anns[ds].itertuples():\n",
    "        if row[2] in Non_beat_anns:\n",
    "            continue\n",
    "        elif row[1] < 91:\n",
    "            continue\n",
    "        elif row[1] + 144 > len(lead0):\n",
    "            continue\n",
    "        else:\n",
    "            anns.append(row[2])\n",
    "            annIdxs.append(row[1] - 1)\n",
    "            hbs0.append(lead0[row[1] - 91: row[1] + 144])\n",
    "            hbs1.append(lead1[row[1] - 91: row[1] + 144])\n",
    "    Hbs[ds] = pd.DataFrame({'lead0': hbs0, 'lead1': hbs1, 'ann': anns, 'annIdx': annIdxs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DS1 recording 114 lead correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DSName == 'DS1':\n",
    "    temp = pd.Series(list(Hbs['114']['lead0']))\n",
    "    Hbs['114']['lead0'] = Hbs['114']['lead1']\n",
    "    Hbs['114']['lead1'] = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing RR Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in DS:\n",
    "    dsLast = len(Hbs[ds]['annIdx']) - 1\n",
    "    preRR = [np.NAN]\n",
    "    postRR = [Hbs[ds]['annIdx'][1] - Hbs[ds]['annIdx'][0]]\n",
    "    for l in range(1, dsLast):\n",
    "        preRR.append(Hbs[ds]['annIdx'][l] - Hbs[ds]['annIdx'][l-1])\n",
    "        postRR.append(Hbs[ds]['annIdx'][l+1] - Hbs[ds]['annIdx'][l])\n",
    "    preRR.append(Hbs[ds]['annIdx'][dsLast] - Hbs[ds]['annIdx'][dsLast-1])\n",
    "    postRR.append(np.NAN)\n",
    "    Hbs[ds]['preRR'] = preRR\n",
    "    Hbs[ds]['postRR'] = postRR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skewness and Kurtosis Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in DS:\n",
    "    dsLength = len(Hbs[ds]['annIdx'])\n",
    "    skewness_0 = []\n",
    "    kurtosis_0 = []\n",
    "    skewness_1 = []\n",
    "    kurtosis_1 = []\n",
    "    for l in range(0, dsLength):\n",
    "        skewness_0.append(Hbs[ds]['lead0'][l].skew())\n",
    "        skewness_1.append(Hbs[ds]['lead1'][l].skew())\n",
    "        kurtosis_0.append(Hbs[ds]['lead0'][l].kurt())\n",
    "        kurtosis_1.append(Hbs[ds]['lead1'][l].kurt())\n",
    "    Hbs[ds]['skewness_0'] = skewness_0\n",
    "    Hbs[ds]['skewness_1'] = skewness_1\n",
    "    Hbs[ds]['kurtosis_0'] = kurtosis_0\n",
    "    Hbs[ds]['kurtosis_1'] = kurtosis_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DWT-based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelet = pywt.Wavelet('haar')\n",
    "for ds in DS:\n",
    "    dsLength = len(Hbs[ds]['annIdx'])\n",
    "    cA7_lead0 = []\n",
    "    cD7_lead0 = []\n",
    "    cD6_lead0 = []\n",
    "    cD5_lead0 = []\n",
    "    cD4_lead0 = []\n",
    "    cD3_lead0 = []\n",
    "    cD2_lead0 = []\n",
    "    cD1_lead0 = []\n",
    "    \n",
    "    cA7_lead1 = []\n",
    "    cD7_lead1 = []\n",
    "    cD6_lead1 = []\n",
    "    cD5_lead1 = []\n",
    "    cD4_lead1 = []\n",
    "    cD3_lead1 = []\n",
    "    cD2_lead1 = []\n",
    "    cD1_lead1 = []\n",
    "    \n",
    "    for l in range(0, dsLength):\n",
    "        beat = list(Hbs[ds]['lead0'][l])\n",
    "        coffs = pywt.wavedec(beat, wavelet, mode='symmetric', level=7)\n",
    "        cA7_lead0.append(list(coffs[0]))\n",
    "        cD7_lead0.append(list(coffs[1]))\n",
    "        cD6_lead0.append(list(coffs[2]))\n",
    "        cD5_lead0.append(list(coffs[3]))\n",
    "        cD4_lead0.append(list(coffs[4]))\n",
    "        cD3_lead0.append(list(coffs[5]))\n",
    "        cD2_lead0.append(list(coffs[6]))\n",
    "        cD1_lead0.append(list(coffs[7]))\n",
    "        \n",
    "        beat = list(Hbs[ds]['lead1'][l])\n",
    "        coffs = pywt.wavedec(beat, wavelet, mode='symmetric', level=7)\n",
    "        cA7_lead1.append(list(coffs[0]))\n",
    "        cD7_lead1.append(list(coffs[1]))\n",
    "        cD6_lead1.append(list(coffs[2]))\n",
    "        cD5_lead1.append(list(coffs[3]))\n",
    "        cD4_lead1.append(list(coffs[4]))\n",
    "        cD3_lead1.append(list(coffs[5]))\n",
    "        cD2_lead1.append(list(coffs[6]))\n",
    "        cD1_lead1.append(list(coffs[7]))\n",
    "    \n",
    "    Hbs[ds]['cA7_0'] = cA7_lead0\n",
    "    Hbs[ds]['cD7_0'] = cD7_lead0\n",
    "    Hbs[ds]['cD6_0'] = cD6_lead0\n",
    "    Hbs[ds]['cD5_0'] = cD5_lead0\n",
    "    Hbs[ds]['cD4_0'] = cD4_lead0\n",
    "    Hbs[ds]['cD3_0'] = cD3_lead0\n",
    "    Hbs[ds]['cD2_0'] = cD2_lead0\n",
    "    Hbs[ds]['cD1_0'] = cD1_lead0\n",
    "    \n",
    "    Hbs[ds]['cA7_1'] = cA7_lead1\n",
    "    Hbs[ds]['cD7_1'] = cD7_lead1\n",
    "    Hbs[ds]['cD6_1'] = cD6_lead1\n",
    "    Hbs[ds]['cD5_1'] = cD5_lead1\n",
    "    Hbs[ds]['cD4_1'] = cD4_lead1\n",
    "    Hbs[ds]['cD3_1'] = cD3_lead1\n",
    "    Hbs[ds]['cD2_1'] = cD2_lead1\n",
    "    Hbs[ds]['cD1_1'] = cD1_lead1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 把lead0和lead1分成2个数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hbs_lead0 = {}\n",
    "Hbs_lead1 = {}\n",
    "\n",
    "for ds in DS:\n",
    "    Hbs_lead0[ds] = pd.DataFrame({\n",
    "        'ann': Hbs[ds]['ann'],\n",
    "        'annIdx': Hbs[ds]['annIdx'],\n",
    "        'beat': Hbs[ds]['lead0'],\n",
    "        'preRR': Hbs[ds]['preRR'],\n",
    "        'postRR': Hbs[ds]['postRR'],\n",
    "        'skewness': Hbs[ds]['skewness_0'],\n",
    "        'kurtosis': Hbs[ds]['kurtosis_0'],\n",
    "        'cA7': Hbs[ds]['cA7_0'],\n",
    "        'cD7': Hbs[ds]['cD7_0'],\n",
    "        'cD6': Hbs[ds]['cD6_0'],\n",
    "        'cD5': Hbs[ds]['cD5_0'],\n",
    "        'cD4': Hbs[ds]['cD4_0'],\n",
    "        'cD3': Hbs[ds]['cD3_0'],\n",
    "        'cD2': Hbs[ds]['cD2_0'],\n",
    "        'cD1': Hbs[ds]['cD1_0'],\n",
    "    })\n",
    "    \n",
    "    Hbs_lead1[ds] = pd.DataFrame({\n",
    "        'ann': Hbs[ds]['ann'],\n",
    "        'annIdx': Hbs[ds]['annIdx'],\n",
    "        'beat': Hbs[ds]['lead1'],\n",
    "        'preRR': Hbs[ds]['preRR'],\n",
    "        'postRR': Hbs[ds]['postRR'],\n",
    "        'skewness': Hbs[ds]['skewness_1'],\n",
    "        'kurtosis': Hbs[ds]['kurtosis_1'],\n",
    "        'cA7': Hbs[ds]['cA7_1'],\n",
    "        'cD7': Hbs[ds]['cD7_1'],\n",
    "        'cD6': Hbs[ds]['cD6_1'],\n",
    "        'cD5': Hbs[ds]['cD5_1'],\n",
    "        'cD4': Hbs[ds]['cD4_1'],\n",
    "        'cD3': Hbs[ds]['cD3_1'],\n",
    "        'cD2': Hbs[ds]['cD2_1'],\n",
    "        'cD1': Hbs[ds]['cD1_1'],\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Extracted Feature to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to solve the problem that the series type can not be correctly stored in the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in DS:\n",
    "    beatValues = []\n",
    "    beatIndexs = []\n",
    "    for i in range(len(Hbs_lead0[ds]['beat'])):\n",
    "        beatValues.append(list(Hbs_lead0[ds]['beat'][i].values))\n",
    "        beatIndexs.append(list(Hbs_lead0[ds]['beat'][i].index))\n",
    "    Hbs_lead0[ds]['beatValues'] = beatValues\n",
    "    Hbs_lead0[ds]['beatIndex'] = beatIndexs\n",
    "    \n",
    "    beatValues = []\n",
    "    beatIndexs = []\n",
    "    for i in range(len(Hbs_lead1[ds]['beat'])):\n",
    "        beatValues.append(list(Hbs_lead1[ds]['beat'][i].values))\n",
    "        beatIndexs.append(list(Hbs_lead1[ds]['beat'][i].index))\n",
    "    Hbs_lead1[ds]['beatValues'] = beatValues\n",
    "    Hbs_lead1[ds]['beatIndex'] = beatIndexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in DS:\n",
    "    Hbs_lead0[ds].to_csv('~/Data/Extracted Features/PLOS_ONE/' + DSName + '/' + ds + '_lead0.csv', index=False)\n",
    "    Hbs_lead1[ds].to_csv('~/Data/Extracted Features/PLOS_ONE/' + DSName + '/' + ds + '_lead1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
